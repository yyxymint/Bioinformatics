{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS470 Introduction to Artificial Intelligence\n",
    "## Deep Learning Practice \n",
    "#### TA. Yechan Hwang\n",
    "---\n",
    "\n",
    "### Agenda for this practice\n",
    "#### 1. Shakespeare dataset\n",
    "#### 2. GRU Model\n",
    "#### 3. Generating texts\n",
    "---\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-1. Text generation with an RNN \n",
    "In this practice, we will learn how to generate text using a character-based RNN. We will practice with a dataset of **Shakespeare's writing** (from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)). We will train a model when given a sequence of characters from this data, that predicts the next character in the sequence. For example, when given the characters 'togethe', trained model will predict 'r' for the next character. Longer sequences of text can be generated by calling the model repeatedly.\n",
    "\n",
    "The following is sample output when the model in this practice trained for 30 epochs, and started with the character 'Q'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<pre>\n",
    "QUEENE:\n",
    "I had thought thou hadst a Roman; for the oracle,\n",
    "Thus by All bids the man against the word,\n",
    "Which are so weak of care, by old care done;\n",
    "Your children were in your holy love,\n",
    "And the precipitation through the bleeding throne.\n",
    "\n",
    "BISHOP OF ELY:\n",
    "Marry, and will, my lord, to weep in such a one were prettiest;\n",
    "Yet now I was adopted heir\n",
    "Of the world's lamentable day,\n",
    "To watch the next way with his father with his face?\n",
    "\n",
    "ESCALUS:\n",
    "The cause why then we are all resolved more sons.\n",
    "\n",
    "VOLUMNIA:\n",
    "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
    "And love and pale as any will to that word.\n",
    "\n",
    "QUEEN ELIZABETH:\n",
    "But how long have I heard the soul for this world,\n",
    "And show his hands of life be proved to stand.\n",
    "\n",
    "PETRUCHIO:\n",
    "I say he look'd on, if I must be content\n",
    "To stay him from the fatal of our country's bliss.\n",
    "His lordship pluck'd from this sentence then for prey,\n",
    "And then let us twain, being the moon,\n",
    "were she such a case as fills m\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While most of the sentences are grammatically correct, they do not make sense. But the model seems to have learned some attributes.\n",
    "\n",
    "- Before the training, the model can't know the style of the training data. \n",
    "- But after training, the structure of the output resembles a playâ€”blocks of text generally begin with a speaker name, in all capital letters similar to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the Shakespeare dataset\n",
    "Run the following lines to download data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.keras/datasets/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data\n",
    "First, let's take a look at the length of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here *length of text* is the number of characters in it. We have more than ten million characters.<br/>\n",
    "Also, we can check the first 250 characters in training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has the format of the screenplay.\n",
    "\n",
    "And how many unique characters are there? Let's check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some special symbols and characters (including lowercase and uppercase letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the text\n",
    "Before training, we need to **map all the characters in the dataset to a numerical representation**. \n",
    "\n",
    "We will create two lookup tables: \n",
    "- one for mapping **characters to numbers**\n",
    "- another for **numbers to characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# 1D integer vector for all characters in the text data\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '$' :   3,\n",
      "  '&' :   4,\n",
      "  \"'\" :   5,\n",
      "  ',' :   6,\n",
      "  '-' :   7,\n",
      "  '.' :   8,\n",
      "  '3' :   9,\n",
      "  ':' :  10,\n",
      "  ';' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'F' :  18,\n",
      "  'G' :  19,\n",
      "  'H' :  20,\n",
      "  'I' :  21,\n",
      "  'J' :  22,\n",
      "  'K' :  23,\n",
      "  'L' :  24,\n",
      "  'M' :  25,\n",
      "  'N' :  26,\n",
      "  'O' :  27,\n",
      "  'P' :  28,\n",
      "  'Q' :  29,\n",
      "  'R' :  30,\n",
      "  'S' :  31,\n",
      "  'T' :  32,\n",
      "  'U' :  33,\n",
      "  'V' :  34,\n",
      "  'W' :  35,\n",
      "  'X' :  36,\n",
      "  'Y' :  37,\n",
      "  'Z' :  38,\n",
      "  'a' :  39,\n",
      "  'b' :  40,\n",
      "  'c' :  41,\n",
      "  'd' :  42,\n",
      "  'e' :  43,\n",
      "  'f' :  44,\n",
      "  'g' :  45,\n",
      "  'h' :  46,\n",
      "  'i' :  47,\n",
      "  'j' :  48,\n",
      "  'k' :  49,\n",
      "  'l' :  50,\n",
      "  'm' :  51,\n",
      "  'n' :  52,\n",
      "  'o' :  53,\n",
      "  'p' :  54,\n",
      "  'q' :  55,\n",
      "  'r' :  56,\n",
      "  's' :  57,\n",
      "  't' :  58,\n",
      "  'u' :  59,\n",
      "  'v' :  60,\n",
      "  'w' :  61,\n",
      "  'x' :  62,\n",
      "  'y' :  63,\n",
      "  'z' :  64,\n",
      "\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(65)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "[18 47 56 ... 45  8  0]\n"
     ]
    }
   ],
   "source": [
    "print(len(text_as_int))\n",
    "print(text_as_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "Also let's check how the first 13 characters from the dataset text are mapped to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The prediction task\n",
    "Our goal is when given a character or a sequence of characters, to predict **the most probable following character.** Therefore, the **input to the model will be a sequence of characters** and the model will learn to **predict the output : the following character at each time step**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training examples and targets\n",
    "For now, divide the text into training sequences. Each input sequence will contain `seq_length` characters from the text. For each input sequence, the corresponding targets contain the same length of text, but shifted one character to the right.\n",
    "\n",
    "So break the text into chunks of `seq_length+1`. For example, let's say that `seq_length` is 4 and our training text is \"HELLO\".\n",
    "\n",
    "In this example, **the input sequence would be \"HELL\", and the target sequence \"ELLO\"**.\n",
    "\n",
    "<img src=\"images/teacher_forcing.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "To do this, first use the [`tf.data.Dataset.from_tensor_slices`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#from_tensor_slices) function to convert the text vector into a stream of character indices.\n",
    "- `tf.data.Dataset.from_tensor_slices`: Creates a Dataset whose elements are slices of the given tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 : F\n",
      "47 : i\n",
      "56 : r\n",
      "57 : s\n",
      "58 : t\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Make char dataset (in the form of integer)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(str(i.numpy())+\" : \"+str(idx2char[i.numpy()]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11043 sequences of length 101\n",
      "\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "101\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "101\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "101\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "101\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "# Make sequences with sequence length +1\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "print(str(len(sequences))+\" sequences of length \"+str(seq_length+1))\n",
    "print()\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    print(len(idx2char[item.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Now we have to convert above text into input data and target data. Note that target data must be shifted one character to the right.\n",
    "\n",
    "To do this, we will use `tf.data.Dataset.map`. When we give some function to `tf.data.Dataset.map` as a parameter, it will apply the function to all elements and then return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "def plus_1(x):\n",
    "    return x+1\n",
    "    \n",
    "temp_dataset = tf.data.Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
    "print(list(temp_dataset.as_numpy_iterator()))\n",
    "temp_dataset = temp_dataset.map(plus_1)\n",
    "print(list(temp_dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "During the training, **each index of these vectors are processed as one time step**. For the input at time step 0, the model receives the index for \"F\" and tries to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the **RNN considers the previous step context in addition to the current input character**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training batches\n",
    "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to **shuffle the data and pack it into batches**.\n",
    "\n",
    "[`tf.data.Dataset.shuffle`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle)(buffer_size, seed=None, reshuffle_each_iteration=None) : Randomly shuffles the elements of this dataset.\n",
    "\n",
    "[`tf.data.Dataset.batch`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#batch)(batch_size, drop_remainder=False) : Combines consecutive elements of this dataset into batches.\n",
    "\n",
    "Note that `tf.data.Dataset.shuffle` **doesn't shuffle characters in each sentence**, but the sentences in dataset will be shuffled by sentences.\n",
    "\n",
    "<img src=\"images/shuffle1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<img src=\"images/shuffle2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Shuffle the data and create batches (1 data = (100, 100) ==> 0:99, 1:100)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "We can see that each batch has 64 input sentences (each has 100 characters) and 64 target sentences (each has 100 characters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[58 46 47 ... 39 58  1]\n",
      " [58  1 51 ... 39 58 56]\n",
      " [42 63  1 ... 59 43  1]\n",
      " ...\n",
      " [43  1 58 ... 46 47 57]\n",
      " [43  1 63 ... 53  1 39]\n",
      " [43 50 47 ... 43 39 56]], shape=(64, 100), dtype=int64) tf.Tensor(\n",
      "[[46 47 57 ... 58  1 47]\n",
      " [ 1 51 63 ... 58 56 43]\n",
      " [63  1 28 ... 43  1 47]\n",
      " ...\n",
      " [ 1 58 46 ... 47 57  1]\n",
      " [ 1 63 53 ...  1 39 52]\n",
      " [50 47 60 ... 39 56  5]], shape=(64, 100), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch, target_example_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Also we can see that all the target sentences are shifted one character to the right ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[52 58 56 39 52 41 43  1 58 53  1 57 59 41 46  1 41 53 51 54 39 52 47 53\n",
      " 52 57 12  0 28 56 39 63  6  1 45 43 58  1 63 53 59  1 53 59 58  8  0  0\n",
      " 15 27 30 21 27 24 13 26 33 31 10  0 13 61 39 63  2  0  0 31 43 41 53 52\n",
      " 42  1 31 43 56 60 47 52 45 51 39 52 10  0 13 61 39 63  2  1 45 43 58  1\n",
      " 63 53 59  1], shape=(100,), dtype=int64) tf.Tensor(\n",
      "[58 56 39 52 41 43  1 58 53  1 57 59 41 46  1 41 53 51 54 39 52 47 53 52\n",
      " 57 12  0 28 56 39 63  6  1 45 43 58  1 63 53 59  1 53 59 58  8  0  0 15\n",
      " 27 30 21 27 24 13 26 33 31 10  0 13 61 39 63  2  0  0 31 43 41 53 52 42\n",
      "  1 31 43 56 60 47 52 45 51 39 52 10  0 13 61 39 63  2  1 45 43 58  1 63\n",
      " 53 59  1 39], shape=(100,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch[0], target_example_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The GRU Model\n",
    "We will use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
    "\n",
    "- `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions;\n",
    "- [`tf.keras.layers.GRU`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU): A type of RNN with size units=rnn_units (You can also use a LSTM layer here.)\n",
    "<img src=https://miro.medium.com/max/2400/1*dhq14CzJijlqjf7IlDB0uw.png>\n",
    "\n",
    " \n",
    "- `tf.keras.layers.Dense`: The output layer, with vocab_size outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About the GRU\n",
    "\n",
    "GRU is a variation of LSTM. GRU has some different attributes compared to vanilla LSTM.\n",
    "\n",
    "- The two state vectors $c_t$ and $h_t$ in the LSTM Cell are merged into one vector $h_t$.\n",
    "- There is only one gate controller $z_t$ that controls all input gates.\n",
    "- There is no output gate and the state vector $h_t$ is the output of GRU.\n",
    "\n",
    "You can see details about the GRU at the this [link](https://arxiv.org/abs/1406.1078).\n",
    "In this practice, we will use GRU since its operation is faster than LSTM and it has fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab) #65\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense \n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim,\n",
    "                  batch_input_shape=[batch_size, None]\n",
    "        ),\n",
    "        \n",
    "        GRU(rnn_units, # Positive integer, dimensionality of the output space.\n",
    "            return_sequences=True,\n",
    "            stateful=True,\n",
    "            \n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        ), # Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "        \n",
    "        Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
    "\n",
    "<img src=https://www.tensorflow.org/tutorials/text/images/text_generation_training.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the untrained model\n",
    "Now we will run the untrained model to see how it behaves.\n",
    "\n",
    "First let's check the shape of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Also we can check the model's prediction by probability distribution. For example, we can see the model's prediction for the first sentence's fifth character.\n",
    "\n",
    "(Note that since the current model is not trained yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(65,), dtype=float32, numpy=\n",
       "array([-0.00655948, -0.00887025, -0.00513276, -0.01366941,  0.000324  ,\n",
       "        0.00285397,  0.00306769, -0.00603657, -0.00159493, -0.00246504,\n",
       "       -0.01265231, -0.00306546,  0.02745044,  0.00645531, -0.00218611,\n",
       "       -0.00160133,  0.0029847 ,  0.01137308, -0.00872085, -0.00084016,\n",
       "       -0.00415169,  0.01291722, -0.00665482,  0.00192988, -0.00057711,\n",
       "        0.01072177, -0.00256717,  0.01129484,  0.00790215, -0.01208447,\n",
       "       -0.01399776, -0.01143389,  0.00929269,  0.00351196, -0.00552383,\n",
       "        0.00270926, -0.01175875,  0.0083756 ,  0.00455535,  0.00360413,\n",
       "        0.00956953, -0.00444007, -0.01121212, -0.00681937,  0.01667031,\n",
       "       -0.00322231, -0.00753562,  0.00932624,  0.01849244, -0.01253507,\n",
       "        0.01506635, -0.0082785 ,  0.00503826, -0.00999888,  0.00415989,\n",
       "       -0.00573193, -0.02312262, -0.00099103, -0.0030157 , -0.01658688,\n",
       "        0.01287276, -0.00634707, -0.01557788,  0.00228248,  0.00176762],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions[0][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in our model, the sequence length of the input is 100 but the model can be run on inputs of any length.\n",
    "\n",
    "This is an advantage of the recurrent neural network which can handle inputs of variable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get actual predictions from the model, we need to sample from the output distribution to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 9]\n",
      " [16]\n",
      " [62]\n",
      " [15]\n",
      " [ 3]\n",
      " [32]\n",
      " [17]\n",
      " [40]\n",
      " [11]\n",
      " [60]\n",
      " [54]\n",
      " [39]\n",
      " [25]\n",
      " [24]\n",
      " [32]\n",
      " [ 6]\n",
      " [21]\n",
      " [25]\n",
      " [37]\n",
      " [46]\n",
      " [17]\n",
      " [52]\n",
      " [39]\n",
      " [31]\n",
      " [29]\n",
      " [57]\n",
      " [33]\n",
      " [36]\n",
      " [50]\n",
      " [ 0]\n",
      " [14]\n",
      " [57]\n",
      " [52]\n",
      " [60]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [31]\n",
      " [ 6]\n",
      " [ 0]\n",
      " [28]\n",
      " [20]\n",
      " [ 8]\n",
      " [37]\n",
      " [50]\n",
      " [37]\n",
      " [55]\n",
      " [21]\n",
      " [44]\n",
      " [46]\n",
      " [13]\n",
      " [57]\n",
      " [ 7]\n",
      " [22]\n",
      " [55]\n",
      " [57]\n",
      " [13]\n",
      " [39]\n",
      " [38]\n",
      " [ 6]\n",
      " [19]\n",
      " [56]\n",
      " [31]\n",
      " [33]\n",
      " [13]\n",
      " [39]\n",
      " [38]\n",
      " [13]\n",
      " [ 2]\n",
      " [54]\n",
      " [29]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [17]\n",
      " [26]\n",
      " [29]\n",
      " [64]\n",
      " [43]\n",
      " [56]\n",
      " [ 6]\n",
      " [19]\n",
      " [47]\n",
      " [ 2]\n",
      " [41]\n",
      " [ 3]\n",
      " [34]\n",
      " [56]\n",
      " [37]\n",
      " [25]\n",
      " [40]\n",
      " [26]\n",
      " [25]\n",
      " [18]\n",
      " [26]\n",
      " [58]\n",
      " [38]\n",
      " [ 3]\n",
      " [45]\n",
      " [59]\n",
      " [36]], shape=(100, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# num_samples : determines how many characters to sample at each iteration\n",
    "\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a prediction for the next character index at each timestep.\n",
    "\n",
    "Now to see the predicted sentence of our untrained model, we will squeeze the `sampled_indices` and convert them into characters.\n",
    "- [`tf.squeeze`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/squeeze): Removes dimensions of size 1 from the shape of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[ 9 16 62 15  3 32 17 40 11 60 54 39 25 24 32  6 21 25 37 46 17 52 39 31\n",
      " 29 57 33 36 50  0 14 57 52 60  2  2 31  6  0 28 20  8 37 50 37 55 21 44\n",
      " 46 13 57  7 22 55 57 13 39 38  6 19 56 31 33 13 39 38 13  2 54 29  2  9\n",
      "  9 17 26 29 64 43 56  6 19 47  2 41  3 34 56 37 25 40 26 25 18 26 58 38\n",
      "  3 45 59 36]\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "print(sampled_indices.shape)\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sqeezing the `sampled_indices`, we got 1D vector that contains indicies of predicted characters.\n",
    "\n",
    "Let's decode this vector to see the text predicted by this untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'r, trust it,\\nHe shall not rule me.\\n\\nANTIGONUS:\\nLa you now, you hear:\\nWhen she will take the rein I l'\n",
      "\n",
      "Next Char Predictions: \n",
      " '3DxC$TEb;vpaMLT,IMYhEnaSQsUXl\\nBsnv!!S,\\nPH.YlYqIfhAs-JqsAaZ,GrSUAaZA!pQ!33ENQzer,Gi!c$VrYMbNMFNtZ$guX'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Since our model is not trained yet, it seems to just predict next character randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "At this point the problem can be treated as a standard classification problem. **Given the previous RNN state and the input character at each time step, our model must predict the next character.**\n",
    "\n",
    "#### Compile the model\n",
    "We will use `tf.keras.losses.sparse_categorical_crossentropy` loss function since it works well for classification problem.\n",
    "\n",
    "Since our model returns logits, we need to set the `from_logits` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.174271\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# Get loss value from untrained model\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure checkpoints\n",
    "Use a [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "172/172 [==============================] - 247s 1s/step - loss: 3.3027\n",
      "Epoch 2/15\n",
      "172/172 [==============================] - 247s 1s/step - loss: 2.0566\n",
      "Epoch 3/15\n",
      "172/172 [==============================] - 247s 1s/step - loss: 1.7441\n",
      "Epoch 4/15\n",
      "172/172 [==============================] - 246s 1s/step - loss: 1.5764\n",
      "Epoch 5/15\n",
      "172/172 [==============================] - 246s 1s/step - loss: 1.4704\n",
      "Epoch 6/15\n",
      "172/172 [==============================] - 246s 1s/step - loss: 1.4032\n",
      "Epoch 7/15\n",
      "172/172 [==============================] - 246s 1s/step - loss: 1.3549\n",
      "Epoch 8/15\n",
      "172/172 [==============================] - 246s 1s/step - loss: 1.3095\n",
      "Epoch 9/15\n",
      "172/172 [==============================] - 247s 1s/step - loss: 1.2722\n",
      "Epoch 10/15\n",
      "172/172 [==============================] - 247s 1s/step - loss: 1.2397\n",
      "Epoch 11/15\n",
      "172/172 [==============================] - 246s 1s/step - loss: 1.2022\n",
      "Epoch 12/15\n",
      "172/172 [==============================] - 247s 1s/step - loss: 1.1700\n",
      "Epoch 13/15\n",
      "172/172 [==============================] - 246s 1s/step - loss: 1.1387\n",
      "Epoch 14/15\n",
      "172/172 [==============================] - 247s 1s/step - loss: 1.1034\n",
      "Epoch 15/15\n",
      "172/172 [==============================] - 246s 1s/step - loss: 1.0670\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, \n",
    "                    epochs=15, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate text\n",
    "We will restore the latest checkpoint. Then, to keep this prediction step simple, we will use 1 for batch size.\n",
    "\n",
    "(To run the model with a different `batch_size`, we need to rebuild the model with different batch size and restore the weights from the checkpoint.)\n",
    "\n",
    "By the codes below, we can check the path that contains weights for the lastest model and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the model by changing batch size (=1) to predict new text\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# Load the weight of the model we trained \n",
    "# model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.load_weights('./saved_ckpt/ckpt_15')\n",
    "\n",
    "# Change the batch size from 64 to 1\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The prediction loop\n",
    "\n",
    "The following code block generates the text:\n",
    "\n",
    "1. Start by choosing a **start string** and initialize the RNN hidden state for the first iteration.\n",
    "2. Set the number of characters to generate.\n",
    "3. Get the **prediction distribution of the next character using the start string and hidden state**.\n",
    "4. Smaple an index of the predicted character using a multinomial distribution of the first iteration. \n",
    "5. Use this predicted character as our next input to the model.\n",
    "6. Repeat step 3-5 until we get the number of characters we set.\n",
    "\n",
    "**Note that the RNN hidden state returned by the model is fed back into the model and hidden state will become more complex as the prediction loop repeats.**\n",
    "In other words, after predicting the a word, the modified RNN states are again fed back into the model, which is how the model learns as it gets more context from the previously predicted words.\n",
    "\n",
    "\n",
    "![To generate text the model's output is fed back to the input](https://www.tensorflow.org/tutorials/text/images/text_generation_sampling.png)\n",
    "\n",
    "Looking at the generated text, you'll see the model knows when to capitalize and make paragraphs, and it imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate,temperature):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUEEN:\n",
      "Ay, but the world go.\n",
      "\n",
      "LUCIO:\n",
      "Why, more strict need not to consider of your company.\n",
      "\n",
      "VIRGILIA:\n",
      "O, \n"
     ]
    }
   ],
   "source": [
    "# Low temperatures results in more predictable text.\n",
    "# Higher temperatures results in more surprising text.\n",
    "# Experiment to find the best setting.\n",
    "\n",
    "print(generate_text(model, start_string=\"QUEEN:\", num_generate = 100, temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "The easiest thing you can do to improve the results is to train it for longer (e.g. try EPOCHS=30).\n",
    "\n",
    "You can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
